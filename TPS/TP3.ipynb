{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35073276-9d89-4914-8cb4-895f5922907b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion  <$10k  $10-20k  $20-30k  $30-40k  $40-50k  $50-75k  \\\n",
      "0            Agnostic     27       34       60       81       76      137   \n",
      "1             Atheist     12       27       37       52       35       70   \n",
      "2            Buddhist     27       21       30       34       33       58   \n",
      "3            Catholic    418      617      732      670      638     1116   \n",
      "4  Don’t know/refused     15       14       15       11       10       35   \n",
      "\n",
      "   $75-100k  $100-150k  >150k  Don't know/refused  \n",
      "0       122        109     84                  96  \n",
      "1        73         59     74                  76  \n",
      "2        62         39     53                  54  \n",
      "3       949        792    633                1489  \n",
      "4        21         17     18                 116  \n",
      "\n",
      " tidy dataset\n",
      "             religion income  count\n",
      "0            Agnostic  <$10k     27\n",
      "1             Atheist  <$10k     12\n",
      "2            Buddhist  <$10k     27\n",
      "3            Catholic  <$10k    418\n",
      "4  Don’t know/refused  <$10k     15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('pew.csv')\n",
    "print(df.head())\n",
    "\n",
    "#tidy dataset avec la méthode melt\n",
    "tidy_df = pd.melt(df, id_vars=['religion'], var_name='income', value_name='count')\n",
    "\n",
    "print(\"\\n tidy dataset\")\n",
    "print(tidy_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46f6655b-536a-48cd-8314-5c6c30cbaedc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country  year   m014   m1524   m2534   m3544   m4554  m5564    m65  mu  \\\n",
      "0        AD  2000    0.0     0.0     1.0     0.0     0.0    0.0    0.0 NaN   \n",
      "1        AE  2000    2.0     4.0     4.0     6.0     5.0   12.0   10.0 NaN   \n",
      "2        AF  2000   52.0   228.0   183.0   149.0   129.0   94.0   80.0 NaN   \n",
      "3        AG  2000    0.0     0.0     0.0     0.0     0.0    0.0    1.0 NaN   \n",
      "4        AL  2000    2.0    19.0    21.0    14.0    24.0   19.0   16.0 NaN   \n",
      "..      ...   ...    ...     ...     ...     ...     ...    ...    ...  ..   \n",
      "196      YE  2000  110.0   789.0   689.0   493.0   314.0  255.0  127.0 NaN   \n",
      "197      YU  2000    NaN     NaN     NaN     NaN     NaN    NaN    NaN NaN   \n",
      "198      ZA  2000  116.0   723.0  1999.0  2135.0  1146.0  435.0  212.0 NaN   \n",
      "199      ZM  2000  349.0  2175.0  2610.0  3045.0   435.0  261.0  174.0 NaN   \n",
      "200      ZW  2000    NaN     NaN     NaN     NaN     NaN    NaN    NaN NaN   \n",
      "\n",
      "      f014   f1524   f2534   f3544  f4554  f5564   f65  fu  \n",
      "0      NaN     NaN     NaN     NaN    NaN    NaN   NaN NaN  \n",
      "1      3.0    16.0     1.0     3.0    0.0    0.0   4.0 NaN  \n",
      "2     93.0   414.0   565.0   339.0  205.0   99.0  36.0 NaN  \n",
      "3      1.0     1.0     1.0     0.0    0.0    0.0   0.0 NaN  \n",
      "4      3.0    11.0    10.0     8.0    8.0    5.0  11.0 NaN  \n",
      "..     ...     ...     ...     ...    ...    ...   ...  ..  \n",
      "196  161.0   799.0   627.0   517.0  345.0  247.0  92.0 NaN  \n",
      "197    NaN     NaN     NaN     NaN    NaN    NaN   NaN NaN  \n",
      "198  122.0  1283.0  1716.0   933.0  423.0  167.0  80.0 NaN  \n",
      "199  150.0   932.0  1118.0  1305.0  186.0  112.0  75.0 NaN  \n",
      "200    NaN     NaN     NaN     NaN    NaN    NaN   NaN NaN  \n",
      "\n",
      "[201 rows x 18 columns]\n",
      "     country  year variable  value\n",
      "0         AD  2000     m014    0.0\n",
      "1         AE  2000     m014    2.0\n",
      "2         AF  2000     m014   52.0\n",
      "3         AG  2000     m014    0.0\n",
      "4         AL  2000     m014    2.0\n",
      "...      ...   ...      ...    ...\n",
      "3211      YE  2000       fu    NaN\n",
      "3212      YU  2000       fu    NaN\n",
      "3213      ZA  2000       fu    NaN\n",
      "3214      ZM  2000       fu    NaN\n",
      "3215      ZW  2000       fu    NaN\n",
      "\n",
      "[3216 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tuberculosis.csv')\n",
    "print(df)\n",
    "\n",
    "melted_df = pd.melt(df, id_vars=['country', 'year'], var_name='variable', value_name='value')\n",
    "print(melted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c796e6a-a7c1-49f4-b3af-c0fe70962476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs uniques dans la colonne 'country': ['AD' 'AE' 'AF' 'AG' 'AL' 'AM' 'AN' 'AO' 'AR' 'AS' 'AT' 'AU' 'AZ' 'BA'\n",
      " 'BB' 'BD' 'BE' 'BF' 'BG' 'BH' 'BI' 'BJ' 'BM' 'BN' 'BO' 'BR' 'BS' 'BT'\n",
      " 'BW' 'BY' 'BZ' 'CA' 'CD' 'CF' 'CG' 'CH' 'CI' 'CK' 'CL' 'CM' 'CN' 'CO'\n",
      " 'CR' 'CU' 'CV' 'CY' 'CZ' 'DE' 'DJ' 'DK' 'DO' 'DZ' 'EC' 'EE' 'EG' 'ER'\n",
      " 'ES' 'ET' 'FI' 'FJ' 'FM' 'FR' 'GB' 'GD' 'GE' 'GH' 'GN' 'GR' 'GT' 'GU'\n",
      " 'GW' 'GY' 'HK' 'HN' 'HR' 'HT' 'HU' 'ID' 'IE' 'IL' 'IN' 'IQ' 'IR' 'IS'\n",
      " 'IT' 'JM' 'JO' 'JP' 'KE' 'KG' 'KH' 'KI' 'KM' 'KN' 'KP' 'KR' 'KW' 'KY'\n",
      " 'KZ' 'LA' 'LB' 'LC' 'LK' 'LR' 'LS' 'LT' 'LU' 'LV' 'LY' 'MA' 'MC' 'MD'\n",
      " 'MG' 'MH' 'MK' 'ML' 'MM' 'MN' 'MO' 'MP' 'MR' 'MS' 'MT' 'MU' 'MV' 'MW'\n",
      " 'MX' 'MY' 'MZ' nan 'NC' 'NE' 'NG' 'NI' 'NL' 'NO' 'NP' 'NR' 'NU' 'NZ' 'OM'\n",
      " 'PA' 'PE' 'PF' 'PG' 'PH' 'PK' 'PL' 'PR' 'PS' 'PT' 'PY' 'QA' 'RO' 'RU'\n",
      " 'RW' 'SA' 'SB' 'SC' 'SD' 'SE' 'SG' 'SI' 'SK' 'SL' 'SM' 'SN' 'SO' 'SR'\n",
      " 'ST' 'SV' 'SY' 'SZ' 'TC' 'TG' 'TH' 'TJ' 'TK' 'TM' 'TN' 'TO' 'TR' 'TT'\n",
      " 'TV' 'TZ' 'UA' 'UG' 'US' 'UY' 'UZ' 'VC' 'VE' 'VG' 'VN' 'VU' 'WS' 'YE'\n",
      " 'YU' 'ZA' 'ZM' 'ZW']\n",
      "Valeurs uniques dans la colonne 'year': [2000]\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les valeurs uniques dans les colonnes \"iso2\" et \"year\"\n",
    "print(\"Valeurs uniques dans la colonne 'country':\", tb_data['country'].unique())\n",
    "print(\"Valeurs uniques dans la colonne 'year':\", tb_data['year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ae871eb-80c3-4f66-87a2-13191db08f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Après la transformation :\n",
      "  country  year  value sex  age\n",
      "0      AD  2000    0.0   m  014\n",
      "1      AE  2000    2.0   m  014\n",
      "2      AF  2000   52.0   m  014\n",
      "3      AG  2000    0.0   m  014\n",
      "4      AL  2000    2.0   m  014\n"
     ]
    }
   ],
   "source": [
    "# Utiliser la méthode melt pour transformer le dataset\n",
    "melted_df = pd.melt(df, id_vars=['country', 'year'], var_name='variable', value_name='value')\n",
    "\n",
    "# la création des variables \"sex\" et \"age\"\n",
    "melted_df[['sex', 'age']] = melted_df['variable'].str.extract(r'(\\D)(\\d+)') #(\\D): Capture un caractère non-numérique\n",
    "#(\\d+) : Capture un ou plusieurs chiffres\n",
    "\n",
    "melted_df.drop(columns=['variable'], inplace=True)\n",
    "\n",
    "print(\"\\nAprès la transformation :\")\n",
    "print(melted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1dc48b2-2c03-46bd-b2c9-fe3f5334e697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant la transformation en tidy dataset :\n",
      "        id  year  month element  d1    d2    d3  d4    d5  d6  ...  d22   d23  \\\n",
      "0  MX17004  2010      1    tmax NaN   NaN   NaN NaN   NaN NaN  ...  NaN   NaN   \n",
      "1  MX17004  2010      1    tmin NaN   NaN   NaN NaN   NaN NaN  ...  NaN   NaN   \n",
      "2  MX17004  2010      2    tmax NaN  27.3  24.1 NaN   NaN NaN  ...  NaN  29.9   \n",
      "3  MX17004  2010      2    tmin NaN  14.4  14.4 NaN   NaN NaN  ...  NaN  10.7   \n",
      "4  MX17004  2010      3    tmax NaN   NaN   NaN NaN  32.1 NaN  ...  NaN   NaN   \n",
      "\n",
      "   d24  d25  d26  d27  d28  d29   d30  d31  \n",
      "0  NaN  NaN  NaN  NaN  NaN  NaN  27.8  NaN  \n",
      "1  NaN  NaN  NaN  NaN  NaN  NaN  14.5  NaN  \n",
      "2  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "3  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "4  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "          id  year  month element  day  temp\n",
      "0    MX17004  2010      1    tmax   d1   NaN\n",
      "1    MX17004  2010      1    tmin   d1   NaN\n",
      "2    MX17004  2010      2    tmax   d1   NaN\n",
      "3    MX17004  2010      2    tmin   d1   NaN\n",
      "4    MX17004  2010      3    tmax   d1   NaN\n",
      "..       ...   ...    ...     ...  ...   ...\n",
      "677  MX17004  2010     10    tmin  d31   NaN\n",
      "678  MX17004  2010     11    tmax  d31   NaN\n",
      "679  MX17004  2010     11    tmin  d31   NaN\n",
      "680  MX17004  2010     12    tmax  d31   NaN\n",
      "681  MX17004  2010     12    tmin  d31   NaN\n",
      "\n",
      "[682 rows x 6 columns]\n",
      "\n",
      "tidy dataset\n",
      "element       id  year  month  day  tmax  tmin\n",
      "0        MX17004  2010      1  d30  27.8  14.5\n",
      "1        MX17004  2010      2  d11  29.7  13.4\n",
      "2        MX17004  2010      2   d2  27.3  14.4\n",
      "3        MX17004  2010      2  d23  29.9  10.7\n",
      "4        MX17004  2010      2   d3  24.1  14.4\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('weather.csv')\n",
    "print(\"Avant la transformation en tidy dataset :\")\n",
    "print(df.head())\n",
    "\n",
    "#Melt pour faire fondre les colonnes\n",
    "melted_df = pd.melt(df, id_vars=['id', 'year', 'month', 'element'], var_name='day', value_name='temp')\n",
    "print(melted_df)\n",
    "\n",
    "#Tidy dataset à l'aide de la méthode pivot_table\n",
    "tidy_df = melted_df.pivot_table(index=['id', 'year', 'month', 'day'], columns='element', values='temp').reset_index()\n",
    "\n",
    "print(\"\\ntidy dataset\")\n",
    "print(tidy_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6826c50-1f7e-43a9-8243-ebebd8f81d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     year            artist                    track  time date.entered  wk1  \\\n",
      "0    2000             2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   87   \n",
      "1    2000           2Ge+her  The Hardest Part Of ...  3:15   2000-09-02   91   \n",
      "2    2000      3 Doors Down               Kryptonite  3:53   2000-04-08   81   \n",
      "3    2000      3 Doors Down                    Loser  4:24   2000-10-21   76   \n",
      "4    2000          504 Boyz            Wobble Wobble  3:35   2000-04-15   57   \n",
      "..    ...               ...                      ...   ...          ...  ...   \n",
      "312  2000       Yankee Grey     Another Nine Minutes  3:10   2000-04-29   86   \n",
      "313  2000  Yearwood, Trisha          Real Live Woman  3:55   2000-04-01   85   \n",
      "314  2000   Ying Yang Twins  Whistle While You Tw...  4:19   2000-03-18   95   \n",
      "315  2000     Zombie Nation            Kernkraft 400  3:30   2000-09-02   99   \n",
      "316  2000   matchbox twenty                     Bent  4:12   2000-04-29   60   \n",
      "\n",
      "      wk2   wk3   wk4   wk5  ...  wk67  wk68  wk69  wk70  wk71  wk72  wk73  \\\n",
      "0    82.0  72.0  77.0  87.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1    87.0  92.0   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "2    70.0  68.0  67.0  66.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "3    76.0  72.0  69.0  67.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "4    34.0  25.0  17.0  17.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "..    ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "312  83.0  77.0  74.0  83.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "313  83.0  83.0  82.0  81.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "314  94.0  91.0  85.0  84.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "315  99.0   NaN   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "316  37.0  29.0  24.0  22.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "     wk74  wk75  wk76  \n",
      "0     NaN   NaN   NaN  \n",
      "1     NaN   NaN   NaN  \n",
      "2     NaN   NaN   NaN  \n",
      "3     NaN   NaN   NaN  \n",
      "4     NaN   NaN   NaN  \n",
      "..    ...   ...   ...  \n",
      "312   NaN   NaN   NaN  \n",
      "313   NaN   NaN   NaN  \n",
      "314   NaN   NaN   NaN  \n",
      "315   NaN   NaN   NaN  \n",
      "316   NaN   NaN   NaN  \n",
      "\n",
      "[317 rows x 81 columns]\n",
      "\n",
      "tidy dataset\n",
      "       year            artist                    track  time date.entered  \\\n",
      "0      2000             2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   \n",
      "1      2000           2Ge+her  The Hardest Part Of ...  3:15   2000-09-02   \n",
      "2      2000      3 Doors Down               Kryptonite  3:53   2000-04-08   \n",
      "3      2000      3 Doors Down                    Loser  4:24   2000-10-21   \n",
      "4      2000          504 Boyz            Wobble Wobble  3:35   2000-04-15   \n",
      "...     ...               ...                      ...   ...          ...   \n",
      "24087  2000       Yankee Grey     Another Nine Minutes  3:10   2000-04-29   \n",
      "24088  2000  Yearwood, Trisha          Real Live Woman  3:55   2000-04-01   \n",
      "24089  2000   Ying Yang Twins  Whistle While You Tw...  4:19   2000-03-18   \n",
      "24090  2000     Zombie Nation            Kernkraft 400  3:30   2000-09-02   \n",
      "24091  2000   matchbox twenty                     Bent  4:12   2000-04-29   \n",
      "\n",
      "       week  rank  \n",
      "0       wk1  87.0  \n",
      "1       wk1  91.0  \n",
      "2       wk1  81.0  \n",
      "3       wk1  76.0  \n",
      "4       wk1  57.0  \n",
      "...     ...   ...  \n",
      "24087  wk76   NaN  \n",
      "24088  wk76   NaN  \n",
      "24089  wk76   NaN  \n",
      "24090  wk76   NaN  \n",
      "24091  wk76   NaN  \n",
      "\n",
      "[24092 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('billboard.csv', encoding='latin1')\n",
    "print(df)\n",
    "# Utiliser la méthode melt pour faire fondre les colonnes\n",
    "melted_df = pd.melt(df, id_vars=['year', 'artist', 'track', 'time', 'date.entered'], var_name='week', value_name='rank')\n",
    "print(\"\\ntidy dataset\")\n",
    "print(melted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a6804ee-91b4-4cdd-9542-1da4d7b948ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       year            artist                    track  time date.entered  \\\n",
      "0      2000             2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   \n",
      "1      2000           2Ge+her  The Hardest Part Of ...  3:15   2000-09-02   \n",
      "2      2000      3 Doors Down               Kryptonite  3:53   2000-04-08   \n",
      "3      2000      3 Doors Down                    Loser  4:24   2000-10-21   \n",
      "4      2000          504 Boyz            Wobble Wobble  3:35   2000-04-15   \n",
      "...     ...               ...                      ...   ...          ...   \n",
      "24087  2000       Yankee Grey     Another Nine Minutes  3:10   2000-04-29   \n",
      "24088  2000  Yearwood, Trisha          Real Live Woman  3:55   2000-04-01   \n",
      "24089  2000   Ying Yang Twins  Whistle While You Tw...  4:19   2000-03-18   \n",
      "24090  2000     Zombie Nation            Kernkraft 400  3:30   2000-09-02   \n",
      "24091  2000   matchbox twenty                     Bent  4:12   2000-04-29   \n",
      "\n",
      "       week  rank  \n",
      "0       wk1  87.0  \n",
      "1       wk1  91.0  \n",
      "2       wk1  81.0  \n",
      "3       wk1  76.0  \n",
      "4       wk1  57.0  \n",
      "...     ...   ...  \n",
      "24087  wk76   NaN  \n",
      "24088  wk76   NaN  \n",
      "24089  wk76   NaN  \n",
      "24090  wk76   NaN  \n",
      "24091  wk76   NaN  \n",
      "\n",
      "[24092 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reformater les variables \"date.entered\", \"week\" et \"rank\"\n",
    "melted_df['date.entered'] = pd.to_datetime(melted_df['date.entered'])\n",
    "print(melted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "434ac18b-9009-4f12-b7dc-b45e37cca996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       year            artist                    track  time date.entered  \\\n",
      "0      2000             2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   \n",
      "1      2000           2Ge+her  The Hardest Part Of ...  3:15   2000-09-02   \n",
      "2      2000      3 Doors Down               Kryptonite  3:53   2000-04-08   \n",
      "3      2000      3 Doors Down                    Loser  4:24   2000-10-21   \n",
      "4      2000          504 Boyz            Wobble Wobble  3:35   2000-04-15   \n",
      "...     ...               ...                      ...   ...          ...   \n",
      "24087  2000       Yankee Grey     Another Nine Minutes  3:10   2000-04-29   \n",
      "24088  2000  Yearwood, Trisha          Real Live Woman  3:55   2000-04-01   \n",
      "24089  2000   Ying Yang Twins  Whistle While You Tw...  4:19   2000-03-18   \n",
      "24090  2000     Zombie Nation            Kernkraft 400  3:30   2000-09-02   \n",
      "24091  2000   matchbox twenty                     Bent  4:12   2000-04-29   \n",
      "\n",
      "       week  rank  \n",
      "0       wk1  87.0  \n",
      "1       wk1  91.0  \n",
      "2       wk1  81.0  \n",
      "3       wk1  76.0  \n",
      "4       wk1  57.0  \n",
      "...     ...   ...  \n",
      "24087  wk76   NaN  \n",
      "24088  wk76   NaN  \n",
      "24089  wk76   NaN  \n",
      "24090  wk76   NaN  \n",
      "24091  wk76   NaN  \n",
      "\n",
      "[24092 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reformater les variables \"date.entered\", \"week\" et \"rank\"\n",
    "melted_df['date.entered'] = pd.to_datetime(melted_df['date.entered'])\n",
    "melted_df['rank'] = pd.to_numeric(melted_df['rank'], errors='coerce')\n",
    "print(melted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b755d759-5193-4ec6-9a08-611747ddd0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Après la fusion des colonnes :\n",
      "   year        artist                    track  time date.entered  week  rank\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26     1  87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02     1  91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08     1  81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21     1  76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15     1  57.0\n",
      "\n",
      "Première partie du dataset :\n",
      "   year        artist                    track  time date.entered  week  rank\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26     1  87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02     1  91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08     1  81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21     1  76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15     1  57.0\n",
      "\n",
      "Deuxième partie du dataset :\n",
      "       year        artist                    track  time date.entered  week  \\\n",
      "12046  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26    39   \n",
      "12047  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02    39   \n",
      "12048  2000  3 Doors Down               Kryptonite  3:53   2000-04-08    39   \n",
      "12049  2000  3 Doors Down                    Loser  4:24   2000-10-21    39   \n",
      "12050  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15    39   \n",
      "\n",
      "       rank  \n",
      "12046   NaN  \n",
      "12047   NaN  \n",
      "12048   9.0  \n",
      "12049   NaN  \n",
      "12050   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Reformater les variables \"date.entered\", \"week\" et \"rank\"\n",
    "melted_df['date.entered'] = pd.to_datetime(melted_df['date.entered'])\n",
    "melted_df['week'] = melted_df['week'].str.extract('(\\d+)', expand=False).astype(int)\n",
    "melted_df['rank'] = pd.to_numeric(melted_df['rank'], errors='coerce')\n",
    "\n",
    "# Afficher les premières lignes du dataset fondu pour inspection\n",
    "print(\"\\nAprès la fusion des colonnes :\")\n",
    "print(melted_df.head())\n",
    "\n",
    "# Diviser le dataset en deux parties\n",
    "# Par exemple, si vous souhaitez diviser le dataset en deux parties égales :\n",
    "middle_index = len(melted_df) // 2\n",
    "part1_df = melted_df.iloc[:middle_index]\n",
    "part2_df = melted_df.iloc[middle_index:]\n",
    "\n",
    "# Afficher les premières lignes de chaque partie pour inspection\n",
    "print(\"\\nPremière partie du dataset :\")\n",
    "print(part1_df.head())\n",
    "\n",
    "print(\"\\nDeuxième partie du dataset :\")\n",
    "print(part2_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d87d8c8-52a7-442d-832f-d52c3dfb176a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premières lignes du dataset 2014 :\n",
      "   rank       name  frequency   sex  year\n",
      "0     1       Noah        837  Male  2014\n",
      "1     2  Alexander        747  Male  2014\n",
      "2     3    William        687  Male  2014\n",
      "3     4    Michael        680  Male  2014\n",
      "4     5       Liam        670  Male  2014\n",
      "\n",
      "Premières lignes du dataset 2015 :\n",
      "   rank       name  frequency   sex  year\n",
      "0     1       Noah        863  Male  2015\n",
      "1     2       Liam        709  Male  2015\n",
      "2     3  Alexander        703  Male  2015\n",
      "3     4      Jacob        650  Male  2015\n",
      "4     5    William        618  Male  2015\n",
      "\n",
      "Premières lignes du dataset fusionné :\n",
      "   rank       name  frequency   sex  year\n",
      "0     1       Noah        837  Male  2014\n",
      "1     2  Alexander        747  Male  2014\n",
      "2     3    William        687  Male  2014\n",
      "3     4    Michael        680  Male  2014\n",
      "4     5       Liam        670  Male  2014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les datasets\n",
    "df_2014 = pd.read_csv('babyNames2014.csv')\n",
    "df_2015 = pd.read_csv('babyNames2015.csv')\n",
    "\n",
    "# Rajouter une colonne pour l'année dans chaque dataset\n",
    "df_2014['year'] = 2014\n",
    "df_2015['year'] = 2015\n",
    "\n",
    "# Afficher les premières lignes de chaque dataset pour inspection\n",
    "print(\"Premières lignes du dataset 2014 :\")\n",
    "print(df_2014.head())\n",
    "\n",
    "print(\"\\nPremières lignes du dataset 2015 :\")\n",
    "print(df_2015.head())\n",
    "\n",
    "# Regrouper les deux datasets en un seul à l'aide de la méthode concat\n",
    "merged_df = pd.concat([df_2014, df_2015])\n",
    "\n",
    "# Afficher les premières lignes du dataset fusionné pour inspection\n",
    "print(\"\\nPremières lignes du dataset fusionné :\")\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2213d8b-1a2a-416a-9a10-7a5ed96cbc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
